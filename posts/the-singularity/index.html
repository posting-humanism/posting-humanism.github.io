<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>Machine Learning and the Singularity | </title>
<meta name=keywords content>
<meta name=description content="What is a singularity? In a black hole, the singularity is the point in the center of the black hole which, once you pass the event horizon, the gravity is so strong that you can&rsquo;t escape reaching the center, the singularity, and trying to talk to your daughter in the past with Morse code and sand. There really is no way around it, so you should probably practice what you are gonna say before you cause a bunch of weird trauma and she doesn&rsquo;t realize what you were trying to send scientific data in the most convoluted way physically possible.">
<meta name=author content>
<link rel=canonical href=https://posting-humanism.github.io/posts/the-singularity/>
<link crossorigin=anonymous href=../../assets/css/stylesheet.min.35cd0f65a15cafa92372b8313deef5960aae04b90ad722f2bbf509eb0468137e.css integrity="sha256-Nc0PZaFcr6kjcrgxPe71lgquBLkK1yLyu/UJ6wRoE34=" rel="preload stylesheet" as=style>
<script defer crossorigin=anonymous src=../../assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5+kdJvBz5iKbt6B5PJI=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://posting-humanism.github.io/favicon.ico>
<link rel=icon type=image/png sizes=16x16 href=https://posting-humanism.github.io/favicon-16x16.png>
<link rel=icon type=image/png sizes=32x32 href=https://posting-humanism.github.io/favicon-32x32.png>
<link rel=apple-touch-icon href=https://posting-humanism.github.io/apple-touch-icon.png>
<link rel=mask-icon href=https://posting-humanism.github.io/safari-pinned-tab.svg>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<meta name=generator content="Hugo 0.90.0">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-QNDJJD62GD"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-QNDJJD62GD',{anonymize_ip:!1})}</script>
<meta property="og:title" content="Machine Learning and the Singularity">
<meta property="og:description" content="What is a singularity? In a black hole, the singularity is the point in the center of the black hole which, once you pass the event horizon, the gravity is so strong that you can&rsquo;t escape reaching the center, the singularity, and trying to talk to your daughter in the past with Morse code and sand. There really is no way around it, so you should probably practice what you are gonna say before you cause a bunch of weird trauma and she doesn&rsquo;t realize what you were trying to send scientific data in the most convoluted way physically possible.">
<meta property="og:type" content="article">
<meta property="og:url" content="https://posting-humanism.github.io/posts/the-singularity/"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2021-08-01T09:22:48-07:00">
<meta property="article:modified_time" content="2021-08-01T09:22:48-07:00"><meta property="og:site_name" content="Posting Humanism">
<meta name=twitter:card content="summary">
<meta name=twitter:title content="Machine Learning and the Singularity">
<meta name=twitter:description content="What is a singularity? In a black hole, the singularity is the point in the center of the black hole which, once you pass the event horizon, the gravity is so strong that you can&rsquo;t escape reaching the center, the singularity, and trying to talk to your daughter in the past with Morse code and sand. There really is no way around it, so you should probably practice what you are gonna say before you cause a bunch of weird trauma and she doesn&rsquo;t realize what you were trying to send scientific data in the most convoluted way physically possible.">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://posting-humanism.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Machine Learning and the Singularity","item":"https://posting-humanism.github.io/posts/the-singularity/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Machine Learning and the Singularity","name":"Machine Learning and the Singularity","description":"What is a singularity? In a black hole, the singularity is the point in the center of the black hole which, once you pass the event horizon, the gravity is so strong that you can\u0026rsquo;t escape reaching the center, the singularity, and trying to talk to your daughter in the past with Morse code and sand. There really is no way around it, so you should probably practice what you are gonna say before you cause a bunch of weird trauma and she doesn\u0026rsquo;t realize what you were trying to send scientific data in the most convoluted way physically possible.","keywords":[],"articleBody":"What is a singularity? In a black hole, the singularity is the point in the center of the black hole which, once you pass the event horizon, the gravity is so strong that you can’t escape reaching the center, the singularity, and trying to talk to your daughter in the past with Morse code and sand. There really is no way around it, so you should probably practice what you are gonna say before you cause a bunch of weird trauma and she doesn’t realize what you were trying to send scientific data in the most convoluted way physically possible.\nWould a technological singularity be, then? A technological singularity actually has nothing to do with that physical definition or any physical definition of a black hole singularity, but it sure does sound cool. The event horizon of a technological singularity is the point we may one day cross when an AI makes a smarter AI in an environment that is uncontrolled enough that we can’t stop it from repeating this process indefinitely. The point is, when we pass this event horizon, we have no idea what happens next. And, whatever it is, we cannot stop it. Now, some losers in the government or pop-science will tell you we should do everything we can to avoid this; the uncertainty being reason enough. But can we? If AI gets advanced enough, and there are enough different groups of people working on it, the odds of someone, eventually, fucking up and crossing the event horizon without us knowing is basically 100%.\nSo what’s the timeline then? Well, people like Elon Musk will tell you it could happen at any second. He has repeatedly made the claim that the AI apocalypse is coming, and that we need to do something about it. Which is strange, because he also says, very often, that he is working on advanced AI to drive his cars. So, if he was so worried about it, why doesn’t he just stop working on his self driving cars and save us from the doom he himself said would come from doing exactly what he is currently doing, hmmmmmmm? Well, the answer is the usual answer: Elon Musk is a conman, a charleton, and a liar. In fact, he’s outdone himself and actually lied twice here. The premise, AND what he claims to be doing are both lies. You thought he was bad for doing exactly what he shouldn’t be doing, but in reality he wasn’t doing it and it wasn’t even a problem to begin with. He really got you this time, you should have seen the look on your face.\nLet’s start with the “AI” he is working on. He is making completely self-driving cars, right? Well, theoretically. I can’t prove he won’t ever make a self driving car by doing what he is currently doing, but I can try to explain why it’s taking so long. You see, most of what we call “AI” in today’s world is made with a technique in computer science known as machine learning. If you look up machine learning, you will read all about “neurons”, “rectified linear units”, and even horribly complicated things like “convolution layers”. Whenever you read about something that is so convoluted it has the word “convolution” in the name, you know you are on the wrong side of the wikipedia. Can this really be? Are we making brains? Can something built in Python really think for itself? Well, it depends on how you define “think”, but probably not.\nWhen we peel back these convoluted layers, what we are really making is algorithms. We are making little machines that can classify data that is shown to them. For example, a cat photo classifier. We can make a machine that can tell us, with reasonable accuracy, if a photo is a cat or not. And we feed them an ungodly amount of data to do so. Basically, we feed the algorithm a bunch of photos of cats, and it can only spit out a yes or a no. If it gets it right, then we tell it that, if wrong, we tell it that too, and slowly the algorithm adjusts its parameters until it spits out the right answer as much as it can. There are plenty of people a lot smarter than me who can explain it better, but that is the jist.\nIs that a “brain”? I mean, maybe. Probably part of one, anyway. And we can make them much more versatile than just classifying cats: we can make them classify people, facial expressions, weather, types of clothes, etc. A common one to practice with is making a classifier that can tell you how expensive a house might be just by looking at it. And, as mentioned above, Elon Musk (and others) is trying to use a combination of these techniques to get a car to drive itself.\nBut is it artificial intelligence? Well it’s…artificial. And it contains…intelligence? Maybe it technically counts? But the problem here lies with the difference between what Nolan’s Interstellar thinks is an AI, and what we are calling an AI.\nIn Interstellar, the robots TARS and CASE are independent beings that can operate without human intervention and even have the ability to joke around with the other crew members. Our AI today isn’t really thinking for itself. Rather, it runs data through a deterministic algorithm that spits out an answer. You could make the argument that on some level our brains do the same thing just on a much larger scale, but that misses the point. We can make decisions. We can see something we have never seen before and try to make sense of it. We can pose a threat. We can extrapolate from one situation and apply it to something we’ve never encountered before. We have goals.\nA machine learning algorithm can’t do any of that. In fact a machine learning algorithm can’t really do anything we don’t want it to do. We are in full control and can only end up hurting people with one of these “AI’s” if we apply them incorrectly. A machine learning algorithm can’t make a decision on something it was not trained by us to decide on, and it might be able to write code, technically, but it can’t bring about a technological singularity (lame).\nThis is why both things Elon Musk has claimed about AI are false: the AI apocalypse isn’t coming anytime soon because the framework we are using prevents it and his model for AI uses that same framework, vastly reducing the potential it has, and most likely meaning it won’t ever be able to fully drive a car.\nIn fact, his “AI” can’t even drive on non-arterial roads very well. There are too many variables, too many potential situations to train for. Not to mention the fact that if a model existed that could drive anywhere, it probably couldn’t run on a computer that would be found in a consumer car.\nDon’t get me wrong, I think we should keep trying (cough and also invest heavily in public transit cough), but we shouldn’t be scared of modern AI because of some singularity it might cause. It won’t, not any time soon anyway. We will need a new model for that to happen, and once we do, I don’t think we should be as scared as people think.\nMachine learning is scary for a completely different reason: surveillance and control. It’s no secret that the main use of AI these days is to track us. The reason machine learning is the most popular approach to AI is because it is so good at tracking us; it is not the best model, but simply the most profitable. Making a self driving car would be cool, but Elon can do a lot more with all that data than just drive our cars.\nSo in summary: Elon is a hack and capitalism is bad.\n","wordCount":"1328","inLanguage":"en","datePublished":"2021-08-01T09:22:48-07:00","dateModified":"2021-08-01T09:22:48-07:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://posting-humanism.github.io/posts/the-singularity/"},"publisher":{"@type":"Organization","name":"","logo":{"@type":"ImageObject","url":"https://posting-humanism.github.io/favicon.ico"}}}</script>
</head>
<body id=top>
<script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script>
<noscript>
<style type=text/css>#theme-toggle,.top-link{display:none}</style>
<style>@media(prefers-color-scheme:dark){:root{--theme:#1d1e20;--entry:#2e2e33;--primary:rgba(255, 255, 255, 0.84);--secondary:rgba(255, 255, 255, 0.56);--tertiary:rgba(255, 255, 255, 0.16);--content:rgba(255, 255, 255, 0.74);--hljs-bg:#2e2e33;--code-bg:#37383e;--border:#333}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style>
</noscript>
<header class=header>
<nav class=nav>
<div class=logo>
<span class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</span>
</div>
<ul id=menu>
<li>
<a href=https://posting-humanism.github.io/ title=home>
<span>home</span>
</a>
</li>
<li>
<a href=https://posting-humanism.github.io/archives/ title=archives>
<span>archives</span>
</a>
</li>
</ul>
</nav>
</header>
<main class=main>
<article class=post-single>
<header class=post-header>
<h1 class=post-title>
Machine Learning and the Singularity
</h1>
<div class=post-meta>August 1, 2021
</div>
</header>
<div class=post-content><p>What is a singularity?
In a black hole, the singularity is the point in the center of the black hole which, once you pass the event horizon, the gravity is so strong that you can&rsquo;t escape reaching the center, the singularity, and trying to talk to your daughter in the past with Morse code and sand.
There really is no way around it, so you should probably practice what you are gonna say before you cause a bunch of weird trauma and she doesn&rsquo;t realize what you were trying to send scientific data in the most convoluted way physically possible.</p>
<p>Would a technological singularity be, then?
A technological singularity actually has nothing to do with that physical definition or any physical definition of a black hole singularity, but it sure does sound cool.
The event horizon of a technological singularity is the point we may one day cross when an AI makes a smarter AI in an environment that is uncontrolled enough that we can&rsquo;t stop it from repeating this process indefinitely.
The point is, when we pass this event horizon, we have no idea what happens next.
And, whatever it is, we cannot stop it.
Now, some losers in the government or pop-science will tell you we should do everything we can to avoid this; the uncertainty being reason enough.
But can we?
If AI gets advanced enough, and there are enough different groups of people working on it, the odds of someone, <em>eventually</em>, fucking up and crossing the event horizon without us knowing is basically 100%.</p>
<p>So what&rsquo;s the timeline then?
Well, people like Elon Musk will tell you it could happen at any second.
He has repeatedly made the claim that the AI apocalypse is coming, and that we need to do something about it.
Which is strange, because he <em>also</em> says, very often, that he is working on advanced AI to drive his cars.
So, if he was so worried about it, why doesn&rsquo;t he just stop working on his self driving cars and save us from the doom he himself said would come from doing exactly what he is currently doing, <em>hmmmmmmm</em>?
Well, the answer is the usual answer: Elon Musk is a conman, a charleton, and a liar.
In fact, he&rsquo;s outdone himself and actually lied twice here.
The premise, AND what he claims to be doing are both lies.
You thought he was bad for doing exactly what he shouldn&rsquo;t be doing, but in reality he wasn&rsquo;t doing it and it wasn&rsquo;t even a problem to begin with.
He really got you this time, you should have seen the look on your face.</p>
<p>Let&rsquo;s start with the &ldquo;AI&rdquo; he is working on.
He is making completely self-driving cars, right?
Well, theoretically.
I can&rsquo;t prove he <em>won&rsquo;t</em> ever make a self driving car by doing what he is currently doing, but I can try to explain why it&rsquo;s taking so long.
You see, most of what we call &ldquo;AI&rdquo; in today&rsquo;s world is made with a technique in computer science known as machine learning.
If you look up machine learning, you will read all about &ldquo;neurons&rdquo;, &ldquo;rectified linear units&rdquo;, and even horribly complicated things like &ldquo;convolution layers&rdquo;.
Whenever you read about something that is so convoluted it has the word &ldquo;convolution&rdquo; in the name, you know you are on the wrong side of the wikipedia.
Can this really be?
Are we making brains?
Can something built in Python really think for itself?
Well, it depends on how you define “think”, but probably not.</p>
<p>When we peel back these convoluted layers, what we are really making is algorithms.
We are making little machines that can classify data that is shown to them.
For example, a cat photo classifier.
We can make a machine that can tell us, with reasonable accuracy, if a photo is a cat or not.
And we feed them an ungodly amount of data to do so.
Basically, we feed the algorithm a bunch of photos of cats, and it can only spit out a yes or a no.
If it gets it right, then we tell it that, if wrong, we tell it that too, and slowly the algorithm adjusts its parameters until it spits out the right answer as much as it can.
There are plenty of people a lot smarter than me who can explain it better, but that is the jist.</p>
<p>Is that a &ldquo;brain&rdquo;?
I mean, maybe.
Probably part of one, anyway.
And we can make them much more versatile than just classifying cats: we can make them classify people, facial expressions, weather, types of clothes, etc.
A common one to practice with is making a classifier that can tell you how expensive a house might be just by looking at it.
And, as mentioned above, Elon Musk (and others) is trying to use a combination of these techniques to get a car to drive itself.</p>
<p>But is it artificial intelligence?
Well it&rsquo;s&mldr;artificial.
And it contains&mldr;intelligence?
Maybe it <em>technically</em> counts?
But the problem here lies with the difference between what Nolan&rsquo;s Interstellar thinks is an AI, and what we are calling an AI.</p>
<p>In Interstellar, the robots TARS and CASE are independent beings that can operate without human intervention and even have the ability to joke around with the other crew members.
Our AI today isn&rsquo;t really <em>thinking</em> for itself.
Rather, it runs data through a deterministic algorithm that spits out an answer.
You could make the argument that on some level our brains do the same thing just on a much larger scale, but that misses the point.
We can make decisions.
We can see something we have never seen before and try to make sense of it.
We can pose a threat.
We can extrapolate from one situation and apply it to something we&rsquo;ve never encountered before.
We have goals.</p>
<p>A machine learning algorithm can&rsquo;t do any of that.
In fact a machine learning algorithm can&rsquo;t really do anything we don&rsquo;t want it to do.
We are in full control and can only end up hurting people with one of these &ldquo;AI&rsquo;s&rdquo; if we apply them incorrectly.
A machine learning algorithm can’t make a decision on something it was not trained by us to decide on, and it might be able to write code, technically, but it can&rsquo;t bring about a technological singularity (lame).</p>
<p>This is why both things Elon Musk has claimed about AI are false: the AI apocalypse isn&rsquo;t coming anytime soon because the framework we are using prevents it and his model for AI uses that same framework, vastly reducing the potential it has, and most likely meaning it won&rsquo;t ever be able to fully drive a car.</p>
<p>In fact, his &ldquo;AI&rdquo; can&rsquo;t even drive on non-arterial roads very well.
There are too many variables, too many potential situations to train for.
Not to mention the fact that if a model existed that <em>could</em> drive anywhere, it probably couldn&rsquo;t run on a computer that would be found in a consumer car.</p>
<p>Don&rsquo;t get me wrong, I think we should keep trying (<em>cough</em> and also invest heavily in public transit <em>cough</em>), but we shouldn&rsquo;t be scared of modern AI because of some singularity it might cause.
It won&rsquo;t, not any time soon anyway.
We will need a new model for that to happen, and once we do, I don&rsquo;t think we should be as scared as people think.</p>
<p>Machine learning is scary for a completely different reason: surveillance and control.
It&rsquo;s no secret that the main use of AI these days is to track us.
The reason machine learning is the most popular approach to AI is because it is so good at tracking us; it is not the best model, but simply the most profitable.
Making a self driving car would be cool, but Elon can do a lot more with all that data than just drive our cars.</p>
<p>So in summary: Elon is a hack and capitalism is bad.</p>
</div>
<footer class=post-footer>
</footer>
</article>
</main>
<footer class=footer>
<span>&copy; 2021 <a href=https://posting-humanism.github.io/></a></span>
<span>
Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a>
</span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)">
<button class=top-link id=top-link type=button accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</button>
</a>
<script>let menu=document.getElementById('menu');menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)},document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script>
</body>
</html>