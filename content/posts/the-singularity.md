---
title: "Machine Learning and the Singularity"
date: 2021-08-01T09:22:48-07:00
draft: false
---

What is a singularity?
In a black hole, the singularity is the point in the center of the black hole which, once you pass the event horizon, the gravity is so strong that you cant escape reaching the center, the singularity, and trying to talk to your daughter in the past with Morse code and sand.
There really is no way around it, so you should probably practice what you are gonna say before you cause a bunch of weird trauma and she doesn't realize what you were trying to send scientific data in the most convoluted way physically possible.

Would a technological singularity be, then?
A technological singularity actually has nothing to do with that physical definition or any physical definition of a black hole singularity, but it sure does sound cool.
The event horizon of a technological singularity is the point we may one day cross when an AI makes a smarter AI in an environment that is uncontrolled enough that we can't stop it from repeating this process indefinitely.
The point is, when we pass this event horizon, we have no idea what happens next.
And, whatever it is, we cannot stop it.
Now, some losers in the government or pop-science will tell you we should do everything we can to avoid this; the uncertainty being reason enough.
But can we?
If AI gets advanced enough, and there are enough different groups of people working on it, the odds of someone, *eventually*, fucking up and crossing the event horizon without us knowing is basically 100%.

So whats the timeline then?
Well, people like Elon Musk will tell you it could happen at any second.
He has repeatedly made the claim that the AI apocolypse is coming, and that we need to do something about it.
Which is strange, because he *also* says, very often, that he is working on advanced AI to drive his cars.
So, if he was so worried about it, why doesn't he just stop working on his self driving cars and save us from the doom he himself said would come from doing exactly what he is currently doing, *hmmmmmmm*?
Well, the answer is the usual answer: Elon Musk is a conman, a charleton, and a liar.
In fact, he's outdone himself and actually lied twice here. 
The premise, AND what he claims to be doing are both lies.
You thought he was bad for doing exactly what he shouldn't be doing, but in reality he wasn't doing it and it wasn't even a problem to begin with.
He really got you this time, you should have seen the look on your face.

Let' start with the "AI" he is working on.
He is making completely self driving cars, right?
Well, theoretically.
I can't prove he *won't* ever make a self driving car by doing what he is currently doing, but I can try to explain why it's taking so long.
You see, most of what we call "AI" in today's world is made with a a technique in computer science known as machine learning.
If you look up machine learning, you will read all about "neurons", "rectified linear units", and even horribly complicated things like "convolution layers".
Whenever you read about something that is so convoluted it has the word "convolution" in the name, you know you are on the wrong side of the wikipedia.
Can this really be?
Are we making brains?
Can something built in Python really think for itself?
Well, it depends on how you define think, but probably not.

When we peel back these convoluted layers, what we are really making is algorithms.
We are making little machines that can classify data that is shown to them.
For example, a cat photo classifier.
We can make a machine that can tell us, with reasonable accuracy, if a photo is a cat or not.
And we feed them an ungodly amount of data to do so.
Basically, we feed the algorithm a bunch of photos of cats, and it can only spit out a yes or a no.
If it gets it right, then we tell it that, if wrong, we tell it that too, and slowly the algothirm adjusts its parameters until it spits out the right answer as much as it can.
There are plenty of people a lot smarter than me who can explain it better, but that is the jist.

Is that a "brain"?
I mean, maybe.
Probably part of one, anyway.
And we can make them much more versitile than just classifying cats: we can make them classify people, facial expressions, weather, types of clothes, etc.
A common one to practice with is making a classfier that can tell you how expesive a house might be just by looking at it.
And, as mentioned above, Elon Musk (and others) is trying to use a combination of these techniques to get a car to drive itself.

But is it artificial intelligence?
Well it's...artificial.
And it contains...intelligence?
Maybe it *technically* counts?
But the problem here lies with the difference between what Nolan's Interstellar thinks is an AI, and what we are calling an AI.

In Interstellar, the robots TARS and CASE are independant beings that can opperate without human intervention and even have the ability to joke around with the other crew members.
Our AI today isn't really *thinking* for itself. 
Rather, it runs data through a deterministic algorithm that spits out an answer.
You could make the arguement that on some level our brains do the same thing just on a much larger scale, but that misses the point.
We can make decisions.
We can see something we have never seen before and try to make sense of it.
We can pose a threat.
We can extropolate from one situation and apply it to something we've never encountered before.
We have goals.

A machine learning algorithm can't do any of that.
In fact a machine learning algorithm can't really do anything we don't want it to do.
We are in full control and can only end up hurting people with one of these "AI's" if we apply them incorrectly.
A machine learning algorithm cant make a decision on something it was not trained by us to decide on, and it might be able to write code, technically, but it can't bring about a technological singularity (lame).

This is why both things Elon Musk has claimed about AI are false: the AI apocolypse isn't coming any time soon because the framework we are using prevents it and his model for AI uses that same framework, vastly reducing the potential it has, and most likely meaning it won't ever be able to fully drive a car.

In fact, his "AI" can't even drive on non-artirial roads very well.
There are too many variables, to many potential situations to train for.
Not to mention the fact that if a model existed that *could* drive anywhere, it probably couldn't run on a computer that would be found in a consumer car.

Don't get me wrong, I think we should keep trying (*cough* and also invest heavily in public transit *cough*), but we shouldn't be scared on modern AI because of some singularity it might cause.
It won't, not any time soon anyway.
We will need a new model for that to happen, and once we do, I don't think we should be as scared as poeple think.

Machine learning is scary for a completely different reason: survaillance and control.
It's no secret that the main use of AI these days it for tracking us.
The reason machine learning is the most popular approach to AI is because it is so good at tracking us; it is not the best model, but simply the most profitable.
Making a self driving car would be cool, but Elon can do a lot more with all that data than just drive our cars.

So in summary: Elon is a hack and capitalism is bad.